

The exceeding growth of big datasets has created a need for using distributed systems and 
accelerators for real-time analytics, including for graph and social network analytics. Social 
networks such Facebook and Twitter are constantly growing  at time of writing have over #### 
members with **** relationships between these members.
The decrease of transistor sizes and the improved power consumption, as part of Moore's laws, has 
brought a new challenge with it - an increased number of faults due to random bit flipping. This 
problem is very likely to become even more challenging as transistor sizes continue to decrease and 
the power envelope restrictions become even tighter. As larger systems, with a higher thread count, 
become ubiquitous so will the problem of power related faults.

In this work, we tackle the problem of faults in a connected component algorithm that is 
propagation based. We modify the parallel algorithm of Shiloach-Vishkin \cite{ShiloachVishkin} and 
make it fault-tolerant by augmenting the data structures used by the algorithm and by adding a 
detection and correction scheme at the end of every iteration. By detecting and correcting the 
error at the end of each iteration we are able to limit the propagation of the error to the 
remainder of the graph in the following iterations. This can prove to be especially important for 
distributed implementations that are communication bound.


In this paper we show that the overhead of our fault tolerant algorithm is not significant. 
Specifically, when comparing the fault tolerant algorithm with the its fault-free counter part for 
a fault-free execution (i.e. there are no faults), the use overhead of our algorithm is on average 
$15\%$. This overhead includes updating the new auxiliary data structure and running the detection 
scheme \footnote{The correction scheme is not executed as no faults are detected.}. When faults do 
occur, the execution time of our fault tolerant algorithm is near constant for small to moderate 
fault rates. For extremely high fault rates, where there is one error in each 64-1024 memory 
operations, we see the overhead increasing by anywhere $2X-4X$ that the no-fault overhead. It is 
well worth noting that systems working at such a high fault rate would be considered highly 
unreliable. Lastly, we show that so long as the fault rate is reasonable (i.e. the system is 
reliable), then our algorithm typically does not increase significantly the number of iterations 
needed to converge to the correct answer. Extremely high error rates can prevent propagation of a
label in a specific iteration, thus adding an iteration to the end.

